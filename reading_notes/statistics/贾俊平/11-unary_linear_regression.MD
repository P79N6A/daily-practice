# 11. 一元线性回归

## 11.2 一元线性回归

### 11.2.1 一元线性回归模型

#### ◉ 回归模型
描述因变量 y 如何依赖自变量 x 和误差项 ɛ 的方程称为 **回归模型**。 只涉及一个自变量的一元线性回归模型可表示为：
```
y = β0 + β1*x + ɛ                                   (11.3)
```
在一元线性回归模型中：
* y 是 x 的线性函数（β0 + β1*x 部分）加上误差项 ɛ。
* β0 + β1*x 反映了 x 的变化而引起的 y 的线性变化（β0 和 β1 被称为模型的参数）；
* ɛ 是被称为误差项的随机变量，反映了除 x 和 y 之间的线性关系之外的随机因素对 y 的影响，是不能由 x 和 y 之间的线性关系所解释的变异性。

式(11.3)称为理论回归模型，这一模型有几点假设：

1. 因变量 y 和自变量 x 之间具有线性关系。
2. 在重复抽样中，自变量 x 的取值是固定的，即假定 x 是非随机的。
3. 误差项 ɛ 是一个期望值为 0 的随机变量，即 E(ɛ)=0。这意味着在式(11.3)中，由于 β0 和 β1 都是常数，所以有 E(β0)=β0，E(β1)=β1。因此对于一个给定的 x 值，y 的期望值为 E(y)=β0+β1*x。这实际上等于假定模型的形式为一条直线。
4. 对于所有的 x 值，ɛ 的方差 𝝈^2。
5. 误差项 ɛ 是一个服从正态分布的随机变量，且独立，即 ɛ~N(0, 𝝈^2)

E(y) 的值随着 x 的不同而变化，但无论 x 怎样变化，ɛ 和 y 的概率分布都是正态分布，并且具有相同的方差。

#### ◉ 回归方程
描述因变量 y 的期望值如何依赖于自变量 x 的方程称为 **回归方程**。一元线性回归方程的形式为：
```
E(y) = β0 + β1*x                                    (11.4)
```
一元线性回归方程的图示是一条直线，因此也称为直线回归方程。其中 β0 是回归直线在 y 轴上的截距，是当 x=0 时 y 的期望值；β1 是直线的斜率，它表示 x 每变动一个 u 久唔久发，y 的平均变动值。

#### ◉ 估计的回归方程
```
ŷ = Ḃ0 + Ḃ1*x                                       (11.5)
```
Ḃ0 是估计的回归直线在 y 轴上的截距；Ḃ1 是直线的斜率，表示 x 的每哟我非常一个单位时，y 的平均变动值。


### 11.2.2 参数的最小二乘估计
对于第 i 个 x 值，估计的回归方程可变化为：
```
ŷ[i] = Ḃ0 + Ḃ1*x[i]                                 (11.6)
```
根据最小二乘法，使
```
∑ (y[i]-ŷ[i])^2 = ∑ (y[i]-Ḃ0-Ḃ1*x)^2                (11.7)
```
最小。令 ```Q = ∑ (y[i]-ŷ[i])^2```，Q 是 Ḃ0 和 Ḃ1 的函数，且最小值总是存在。
根据微积分的极值定理，对 Q 求相应于 Ḃ0 和 Ḃ1 的偏导数，并令其等于 0，便可求出 Ḃ0 和 Ḃ1，即：
```
∂S/∂Ḃ0 = -2 ∑[i=1,n] (y[i]]-Ḃ0-Ḃ1*x)^2 = 0          (11.8)
∂S/∂Ḃ1 = -2 ∑[i=1,n] x[i] * (y[i]]-Ḃ0-Ḃ1*x)^2 = 0
```
解上述方程组得：
```
Ḃ1 = (n*∑[i=1,n] x[i]*y[i] - ∑[i=1,n] x[i] *  ∑[i=1,n] y[i]) / (n*[i=1,n] x[i]^2 - ∑[i=1,n]x[i]^2)
Ḃ0 = y_ - Ḃ1*x_                                     (11.9)
```
#### ◉ 最小二乘法
> 最小二乘法（又称最小平方法）是一种数学优化技术。它通过最小化误差的平方和寻找数据的最佳函数匹配。
[示例](https://zh.wikipedia.org/wiki/%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95#.E7.A4.BA.E4.BE.8B)

#### ◉ 偏导数计算方法
对多变量方程中某变量求偏导数，意味着将其它变量视为常数。如：
```
z = f(x, y) = x^2 + x*y + y^2
对 x 量求偏导数，其结果为
∂z/∂x = 2*x + y
```

#### ◉ 正态分布密度函数
```
f(x) = 1/(𝝈*sqrt(2*π)) * e^(-(x-𝜇)^2 / 2𝝈^2)
```

### 11.2.3 回归模型的拟合优度
回归直线 ```ŷ = Ḃ0 + Ḃ1*x ``` 在一定程度上描述了变量 x 与 y 之间的数量关系，根据这一方程，可依据自变量 x 的取值来估计或预测因变量 y 的取值。
但估计或预测的精度如何将取决于回归直线对观测数据的拟合优度。

回归直线与各观测点的接近程度称为回归直线对数据的 **拟合优度**（goodness of fit）。

#### ◉ 判定系数
对一个具体的观测值来说，变差的大小用实际观测值 y 与其均值 y_ 之差 (y-y_) 来表示。
而 n 次观测值的总变差可由这些离差的平方和来表示，称为总平方和，记为 SST，即
```
SST = ∑(y[i]-y_)^2
```
可得
```
∑(y[i]-y_)^2 = ∑(y[i]-ŷ)^2 + ∑(ŷ-y_)^2
```
其中：
* ```∑(y[i]-ŷ)^2```反映了 y 的总变差中 x 与 y 之间的线性关系引起的 y 的变化部分，它是可以由回归直线来解释的 y[i] 的变差部分同，称为**回归平方和**，记为 **SSR**
* ```∑(ŷ-y_)^2``` 是各实际观测点和回归值的残差 ```(ŷ-y_)``` 平方和，它是除了 x 和 y 的线性影响之外的其他因素引起的 y 的变化部分，是不能由回归直线来解释的 y[i] 的变差部分，称为**残差平方**或**误差平方和**，记为 **SSE**。

三个平方和的关系为：
```
总平方和（SST） = 回归平方和(SSR) + 误差平方和（SSE）         (11.14)
```

SSR/SST 越大，直线拟合越好。
回归平方和占总平方和的比例称为 **判定系数** (coefficient of determination)，记为 R^2，其计算公式为：
```
R^2 = SSR/SST = ∑(ŷ-y_)^2 / ∑(y[i]-y_)^2
    = 1 - (∑(y[i]-ŷ)^2 / ∑(y[i]-y_)^2)                  (11.15)
```

* 残差平方和 SSE=0，则 R^2=1，拟合是完全；
* 如果 y 的变化与 x 无关，x 完全无助于解释 y 的变差，此时 ŷ=y_，则 R^2=0

**相关系数** r 为判定系数的 平方根。
|r| 越接近 1，表明回归直线对观测数据的拟合程度越好。

#### ◉ 估计标准误差
**估计标准误差**（standard error of estimate）就是度量各实际观测点在直线周围的散布状况的一个统计量，它是均方残差 (MSE) 的平方差，用 s[e] 来表示：
```
s[e] = sqrt(∑(y[i]-ŷ[i])^2 / (n-2)) = sqrt(SSE/(n-2)) = sqrt(MSE)
```


### 11.2.4 显著性检验
根据数据拟合回归方程，假定变量 x 与 y 之间存在线性关系，即 ```y = β0 + β1*x + ɛ```，并假定误差项 ɛ 是一个服从正态分布的随机变量，且对不同的 x 具有相同的方差。
但是此假设是否成立，需要通过检验才能证实。

回归分析中的显著性检验主要包含两个方面内容：
1. 线性关系检验
2. 回归系数检验

#### ◉ 线性关系检验
将 SSR (回归平方和) 除以相对应的自由度 (SSR 的自由度是自由变量的个数 k，一元线性回归中自由度为 1) 后的结果称为均方回归，记为 MSR；
将 SSE (残差平方和) 除以其相应的自由度（SSE 的自由度为 n-k-1，一元线性回归中自由度为 n-2）后的结果称为均方残差，记为 MSE。

如果原假设成立（H0:B1=0，两个变量之间的线性关系不显著），则比值 MSR/MSE 的抽样分布服务分子自由度为 1、分母自由度为 n-2 的 F 分布，即：
```
F = (SSR/1)/(SSE/(n-1)) = MSR/MSE ~ F(1, n-2)               (11.17)
```

#### ◉ 回归系数检验
